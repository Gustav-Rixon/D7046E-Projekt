{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f6b1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eriks\\AppData\\Local\\Temp/ipykernel_42744/4016041138.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_ = df_.append({\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
    "        }, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "if __name__ == \"__main__\":\n",
    "    # get data, pre-process and split\n",
    "    data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4512759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our own dataset to load the BoW embedded texts\n",
    "class BoWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentence_vectors, labels):\n",
    "        self.sentence_vectors = sentence_vectors\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.labels[index], self.sentence_vectors[index]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1ef85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 [3/3] - Loss: 0.036428036896143e-10"
     ]
    }
   ],
   "source": [
    "bow_train_data = BoWDataset(train_x_tensor, train_y_tensor)\n",
    "bow_test_data = BoWDataset(validation_x_tensor, validation_y_tensor)\n",
    "bow_trainloader = DataLoader(bow_train_data, batch_size=300, shuffle=True)\n",
    "bow_testloader = DataLoader(bow_test_data, batch_size=100, shuffle=False)\n",
    "\n",
    "## CREATE A NEURAL NETWORK TO TRAIN\n",
    "network = nn.Sequential(\n",
    "    # HINT: We want our input to be the size of our BoW embedding and our output to be the different possible classes\n",
    "    nn.Linear(vocab_size, 1000),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1000, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 2)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.1)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_nr, (labels, data) in enumerate(bow_trainloader):\n",
    "        prediction = network(data)\n",
    "        loss = loss_function(prediction, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Print the epoch, batch, and loss\n",
    "        print(\n",
    "            f'\\rEpoch {epoch+1} [{batch_nr+1}/{len(bow_trainloader)}] - Loss: {loss}',\n",
    "            end=''\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff0fd3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the network is 84.0%.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    classes = [0, 1]\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    # For each batch of testing data (since the dataset is too large to run all data through the network at once)\n",
    "    # Calculate the accuracy\n",
    "    for batch_nr, (labels, data) in enumerate(bow_testloader):\n",
    "        prediction = torch.argmax(network(data), dim=-1)\n",
    "            \n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i] == labels[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            y_pred.append(prediction[i])\n",
    "            y_true.append(labels[i])\n",
    "            \n",
    "    print(f'The accuracy of the network is {str(100*correct/total)[:4]}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5386c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'validation_x_tensor.size(0): {train_x_tensor.size(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = word_vectorizer.transform(validation_data)\n",
    "validation_data = validation_data.todense()\n",
    "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2deff055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_tensor.size(0): 900\n",
      "train_x_tensor.size(1): 7305\n",
      "First word index 82: 0.1396804302930832\n",
      "First word index 97: 0.20427875220775604\n",
      "First word index 150: 0.20427875220775604\n",
      "First word index 152: 0.21663209795951843\n",
      "First word index 230: 0.0694056823849678\n",
      "First word index 367: 0.19551390409469604\n",
      "First word index 814: 0.21663209795951843\n",
      "First word index 815: 0.21663209795951843\n",
      "First word index 1457: 0.20427875220775604\n",
      "First word index 1458: 0.21663209795951843\n",
      "First word index 1656: 0.21663209795951843\n",
      "First word index 1657: 0.21663209795951843\n",
      "First word index 2074: 0.15143045783042908\n",
      "First word index 2143: 0.09566876292228699\n",
      "First word index 2195: 0.14092417061328888\n",
      "First word index 4172: 0.09744404256343842\n",
      "First word index 4183: 0.21663209795951843\n",
      "First word index 4752: 0.21663209795951843\n",
      "First word index 4753: 0.21663209795951843\n",
      "First word index 5192: 0.16469334065914154\n",
      "First word index 5194: 0.21663209795951843\n",
      "First word index 5850: 0.179941326379776\n",
      "First word index 5924: 0.16759717464447021\n",
      "First word index 6017: 0.21663209795951843\n",
      "First word index 6039: 0.16759717464447021\n",
      "First word index 6181: 0.07870111614465714\n",
      "First word index 6183: 0.21663209795951843\n",
      "First word index 6279: 0.1373356580734253\n",
      "First word index 6280: 0.21663209795951843\n"
     ]
    }
   ],
   "source": [
    "print(f'train_x_tensor.size(0): {train_x_tensor.size(0)}')\n",
    "print(f'train_x_tensor.size(1): {train_x_tensor.size(1)}')\n",
    "\n",
    "for i, each in enumerate(train_x_tensor[0]):\n",
    "    if each > 0.0:\n",
    "        print(f'First word index {i}: {each}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a6c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
